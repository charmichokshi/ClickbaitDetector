{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538eba51",
   "metadata": {},
   "source": [
    "#### Clickbait Detection on Headline's text & Article's text using Deberta and Electra pre-trained Models and their Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991c9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, balanced_accuracy_score, mean_squared_error, confusion_matrix, recall_score, precision_score, accuracy_score, log_loss\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig,Trainer, TrainingArguments, BertweetTokenizer, BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c62dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Webis17HeadlineArticleClsData/'  # Webis17HeadlineClsData or Webis17HeadlineArticleClsData\n",
    "root = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e66f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path+'train.csv')\n",
    "df_validation = pd.read_csv(path+'validation.csv')\n",
    "df_test = pd.read_csv(path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd7717a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postText</th>\n",
       "      <th>targetParagraphs</th>\n",
       "      <th>truthClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK’s response to modern slavery leaving victim...</td>\n",
       "      <td>Thousands of modern slavery victims have not c...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is good</td>\n",
       "      <td>President Donald Trump has appointed the pro-l...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The \"forgotten\" Trump roast: Relive his brutal...</td>\n",
       "      <td>When the White House correspondents’ dinner is...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meet the happiest #dog in the world!</td>\n",
       "      <td>Adorable is probably an understatement. This a...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tokyo's subway is shut down amid fears over an...</td>\n",
       "      <td>One of Tokyo's major subways systems says it s...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            postText  \\\n",
       "0  UK’s response to modern slavery leaving victim...   \n",
       "1                                       this is good   \n",
       "2  The \"forgotten\" Trump roast: Relive his brutal...   \n",
       "3               Meet the happiest #dog in the world!   \n",
       "4  Tokyo's subway is shut down amid fears over an...   \n",
       "\n",
       "                                    targetParagraphs    truthClass  \n",
       "0  Thousands of modern slavery victims have not c...  no-clickbait  \n",
       "1  President Donald Trump has appointed the pro-l...     clickbait  \n",
       "2  When the White House correspondents’ dinner is...  no-clickbait  \n",
       "3  Adorable is probably an understatement. This a...     clickbait  \n",
       "4  One of Tokyo's major subways systems says it s...  no-clickbait  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb618e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19538, 2459, 18979)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_validation), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0831db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(sentence, replace_url=False, replace_usr=False):\n",
    "    if not replace_url and not replace_usr:\n",
    "        #print(replace_url,replace_usr)\n",
    "        return sentence\n",
    "    new_sentence = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        # print(word)\n",
    "        # @mentions, @users\n",
    "        if (word.startswith(\"@\") or word.startswith('\"@')) and replace_usr:\n",
    "            new_sentence.append('@user')\n",
    "        # URL: https, http\n",
    "        elif (word.startswith(\"http:\") or word.startswith(\"https:\")) and replace_url:\n",
    "            new_sentence.append('internet_site')\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    # remove extra \" \"\n",
    "    new_sentence = re.sub(' +', ' ', \" \".join(new_sentence))\n",
    "    return new_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565ae6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of train 19538\n",
      "Total amount of validation 2459\n",
      "Total amount of test 18979\n",
      "Labels: ['clickbait', 'no-clickbait']\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(df_train,df_dev,df_test,root,replace_url=False,replace_usr=False):\n",
    "    trainDataset = df_train[['postText', 'targetParagraphs', 'truthClass']].copy()\n",
    "    trainDataset['x'] = trainDataset.postText.astype(str).apply(lambda a: normalize_sentence(a,replace_url,replace_usr))\n",
    "    trainDataset['x1'] = trainDataset.targetParagraphs.astype(str).apply(lambda a: normalize_sentence(a,replace_url,replace_usr))\n",
    "    trainDataset['y'] = trainDataset.truthClass.str.strip()\n",
    "    print('Total amount of train',len(trainDataset.index))\n",
    "\n",
    "    validationDataset = df_dev[['postText', 'targetParagraphs', 'truthClass']].copy()\n",
    "    validationDataset['x'] = validationDataset.postText.astype(str).apply(lambda a: normalize_sentence(a,replace_url,replace_usr))\n",
    "    validationDataset['x1'] = trainDataset.targetParagraphs.astype(str).apply(lambda a: normalize_sentence(a,replace_url,replace_usr))\n",
    "    validationDataset['y'] = validationDataset.truthClass.str.strip()\n",
    "    print('Total amount of validation',len(validationDataset.index))\n",
    "\n",
    "    testDataset = df_test[['postText', 'targetParagraphs', 'truthClass']].copy()\n",
    "    testDataset['x'] = testDataset.postText.astype(str).apply(lambda a: normalize_sentence(a,replace_url,replace_usr))\n",
    "    testDataset['x1'] = trainDataset.targetParagraphs.astype(str).apply(lambda a: normalize_sentence(a,replace_url,replace_usr))\n",
    "    testDataset['y'] = testDataset.truthClass.str.strip()\n",
    "    print('Total amount of test',len(testDataset.index))\n",
    "\n",
    "    return trainDataset, validationDataset, testDataset\n",
    "\n",
    "trainDataset, validationDataset, testDataset = get_dataset(df_train, df_validation, df_test, \".\", True, True)\n",
    "\n",
    "labels = list(set(trainDataset.truthClass.tolist()))\n",
    "labels.sort(key=lambda item: (-len(item), item), reverse=True)\n",
    "nLabels = len(labels)\n",
    "print('Labels:', labels)\n",
    "\n",
    "trainDataset = trainDataset.drop(['postText', 'targetParagraphs', 'truthClass'], axis=1)\n",
    "validationDataset = validationDataset.drop(['postText', 'targetParagraphs', 'truthClass'], axis=1)\n",
    "testDataset = testDataset.drop(['postText', 'targetParagraphs', 'truthClass'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1886c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   x  \\\n",
      "0  Johnny Manziel on Browns' No. 1 pick Myles Gar...   \n",
      "1  Fabio: California Is a 'Mess' Because of Liber...   \n",
      "2            \"He's been huge for us this year, man.\"   \n",
      "3  New Bears quarterback Mitchell Trubisky was gr...   \n",
      "4  It's not enough to let employees work flexible...   \n",
      "\n",
      "                                                  x1             y  \n",
      "0  Thousands of modern slavery victims have not c...  no-clickbait  \n",
      "1  President Donald Trump has appointed the pro-l...  no-clickbait  \n",
      "2  When the White House correspondents’ dinner is...     clickbait  \n",
      "3  Adorable is probably an understatement. This a...  no-clickbait  \n",
      "4  One of Tokyo's major subways systems says it s...  no-clickbait  \n"
     ]
    }
   ],
   "source": [
    "print(testDataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c845012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(Trainer):\n",
    "    def evaluate(self, eval_dataset= None, ignore_keys=None):\n",
    "        outputMetrics = super().evaluate(eval_dataset, ignore_keys)\n",
    "#         print('outputMetrics: ', outputMetrics)\n",
    "        return outputMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ec7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gn_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data,labels,tokenizer):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def processText(self, text, text_pair):\n",
    "        tokenized = self.tokenizer(text, text_pair, truncation=True)\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.index)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        row = self.data.iloc[i]\n",
    "        x = self.processText(row['x'], row['x1']).data\n",
    "        \n",
    "        try:\n",
    "            y = self.labels.index(row['y'])\n",
    "        except:\n",
    "            y = len(self.labels) - 1 \n",
    "\n",
    "        x['labels'] = y\n",
    "        return x\n",
    "\n",
    "    def randomItem(self):\n",
    "        return self.__getitem__(random.randint(0,self.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beebf692",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_computeMetrics = []\n",
    "test_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36690d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,modelPath = '',nLabels = 3, labels=None):\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # put cpu for more extensive error desc...\n",
    "        print('device: ', self.device)\n",
    "        self.maxLength = 128  # max 512\n",
    "\n",
    "        self.nLabels = nLabels\n",
    "\n",
    "        self.loadModel(modelPath) \n",
    "        print('** Model: ', self.loadModel)\n",
    "\n",
    "        self.labels = labels\n",
    "\n",
    "    def model_init(self, dropout = 0.1):\n",
    "        config = AutoConfig.from_pretrained(\n",
    "                    self.MODEL_PATH,\n",
    "                    num_labels=self.nLabels,\n",
    "                    return_dict = True,\n",
    "                    hidden_dropout_prob = dropout\n",
    "                )\n",
    "\n",
    "        print('configconfig', config)\n",
    "        return AutoModelForSequenceClassification.from_pretrained(self.MODEL_PATH,config=config).to(self.device)\n",
    "\n",
    "    def computeMetrics(self,evalPrediction):\n",
    "        yPred = evalPrediction.predictions.argmax(1)\n",
    "        yTrue = evalPrediction.label_ids\n",
    "\n",
    "        metrics = {}\n",
    "        \n",
    "        metrics['accuracy'] = accuracy_score(yTrue, yPred)\n",
    "        metrics['f1'] = f1_score(yTrue, yPred)\n",
    "\n",
    "        # original paper: https://link.springer.com/chapter/10.1007/978-3-319-30671-1_72/tables/2 / ROC-AUC, Precision, Recall\n",
    "        metrics['balanced_accuracy'] = balanced_accuracy_score(yTrue, yPred) # deal with imbalanced datasets\n",
    "        metrics['f1_macro'] = f1_score(yTrue, yPred, average='macro') # Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "        metrics['mean_squared_error'] = mean_squared_error(yTrue, yPred)\n",
    "        if 'click' in self.labels[0]:\n",
    "            indexes = [i for i,x in enumerate(self.labels) if x == 'clickbait']\n",
    "            metrics['f1_binary'] = f1_score(yTrue, yPred, average='binary',pos_label=indexes[0])\n",
    "        metrics['confusion_matrix'] = str(confusion_matrix(yTrue, yPred))\n",
    "        tn, fp, fn, tp = confusion_matrix(yTrue, yPred).ravel()\n",
    "        recall = recall_score(yTrue, yPred) \n",
    "        precision = precision_score(yTrue, yPred)\n",
    "        metrics['precision'] = precision\n",
    "        metrics['recall'] = recall\n",
    "        metrics['log_loss'] = log_loss(yTrue, yPred)\n",
    "        \n",
    "        all_computeMetrics.append(metrics)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def loadModel(self,modelPath):\n",
    "        self.MODEL_PATH = modelPath\n",
    "        self.MODEL = self.model_init()\n",
    "        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_PATH,model_max_length = self.maxLength, use_fast=True)\n",
    "        \n",
    "    def saveModel(self,saveDir):\n",
    "        print('saving model at: ', saveDir)\n",
    "        self.MODEL.save_pretrained(saveDir)\n",
    "        self.TOKENIZER.save_pretrained(saveDir)\n",
    "\n",
    "    def train_loop(self,saveDir,checkpointDir,trainingData,validationData,testData,labels):\n",
    "        trainDataset = gn_dataset(trainingData,labels,self.TOKENIZER)\n",
    "        validationDataset = gn_dataset(validationData,labels,self.TOKENIZER)\n",
    "        testDataset = gn_dataset(testData,labels,self.TOKENIZER)\n",
    "        checkpoints = {}\n",
    "\n",
    "        # TRAIN FUNCTION\n",
    "        runName = experiment + str(len(checkpoints))\n",
    "        print('checkpointDir: ', checkpointDir)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "                                output_dir=checkpointDir,\n",
    "                                save_strategy='no',\n",
    "                                do_train=True, # DEFAULT: False\n",
    "                                do_eval=True, # DEFAULT: False\n",
    "                                do_predict=True, # DEFAULT: False\n",
    "#                                 save_steps = 1200, # to save model checkpoints
    "                                eval_steps = 400, #200, # DEFAULT: 500\n",
    "                                evaluation_strategy = 'steps', # DEFAULT: \"no\"\n",
    "                                logging_first_step = True, # DEFAULT: False\n",
    "                                dataloader_num_workers = 12, # 6, DEFAULT: 0\n",
    "                                learning_rate = 2e-5,\n",
    "                                num_train_epochs = 3,\n",
    "                                per_device_train_batch_size = 32,\n",
    "                                per_device_eval_batch_size = 32, #16,\n",
    "                                weight_decay = 0.05 ,\n",
    "                                warmup_steps = 0, # DEFAULT: 0\n",
    "                                logging_dir=checkpointDir+\"/logs\",\n",
    "                                metric_for_best_model = 'f1_binary', # DEFAULT: None\n",
    "                                greater_is_better = True, # DEFAULT: None\n",
    "                                \n",
    "        )\n",
    "        model = self.model_init(dropout=0.3)\n",
    "\n",
    "        trainer = Trainer(\n",
    "                    model,\n",
    "                    args = args,\n",
    "                    train_dataset = trainDataset,#train\n",
    "                    tokenizer = self.TOKENIZER,\n",
    "                    eval_dataset = validationDataset,#dev\n",
    "                    compute_metrics = self.computeMetrics,\n",
    "                )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        cp = checkpointDir + '/CheckPointModel_'+ runName\n",
    "\n",
    "        checkpoints[cp] = trainer.evaluate(testDataset) # on test set\n",
    "        print('on test set:', checkpoints[cp]) \n",
    "        test_metrics.append(checkpoints[cp])\n",
    "#         trainer.predict(testDataset)\n",
    "#         trainer.save_model(cp)\n",
    "        \n",
    "    def predict(self,text):\n",
    "        dataset = gn_dataset(None,None,self.TOKENIZER)\n",
    "        batchEncoding = dataset.processText(text).to(self.device)\n",
    "\n",
    "        self.MODEL.eval()\n",
    "        out = self.MODEL(batchEncoding.input_ids,attention_mask = batchEncoding.attention_mask,token_type_ids = batchEncoding.token_type_ids,return_dict = True)\n",
    "\n",
    "        return out.logits.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d586eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(name,root,nLabels,labels,experiment,trainDataset,validationDataset,testDataset): \n",
    "    print('name experiment: ', name, experiment)\n",
    "    model = Model(name,nLabels,labels)\n",
    "    print('model model: ', model)\n",
    "    analysis = model.train_loop(root+\"SavedModel/\"+experiment,root+name.split('/')[-1]+\"TrainingCheckpoints\",trainDataset,validationDataset,testDataset,labels)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "735de030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name experiment:  google/electra-base-discriminator electra_clickbait\n",
      "device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/charmichokshi4444/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/charmichokshi4444/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configconfig ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /home/charmichokshi4444/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/charmichokshi4444/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/charmichokshi4444/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/charmichokshi4444/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/charmichokshi4444/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Model:  <bound method Model.loadModel of <__main__.Model object at 0x7fe80c390240>>\n",
      "model model:  <__main__.Model object at 0x7fe80c390240>\n",
      "checkpointDir:  ./electra-base-discriminatorTrainingCheckpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/charmichokshi4444/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/charmichokshi4444/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configconfig ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/charmichokshi4444/venv/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 19538\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1833\n",
      "  Number of trainable parameters = 109483778\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1833' max='1833' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1833/1833 07:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "      <th>F1 Binary</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.763100</td>\n",
       "      <td>8.245046</td>\n",
       "      <td>0.605195</td>\n",
       "      <td>0.761285</td>\n",
       "      <td>0.835712</td>\n",
       "      <td>0.688582</td>\n",
       "      <td>0.699641</td>\n",
       "      <td>0.238715</td>\n",
       "      <td>0.563569</td>\n",
       "      <td>[[ 379  383]\n",
       " [ 204 1493]]</td>\n",
       "      <td>0.795842</td>\n",
       "      <td>0.879788</td>\n",
       "      <td>6.059200</td>\n",
       "      <td>405.829000</td>\n",
       "      <td>12.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>7.711293</td>\n",
       "      <td>0.568145</td>\n",
       "      <td>0.776739</td>\n",
       "      <td>0.845309</td>\n",
       "      <td>0.710985</td>\n",
       "      <td>0.722143</td>\n",
       "      <td>0.223261</td>\n",
       "      <td>0.598977</td>\n",
       "      <td>[[ 410  352]\n",
       " [ 197 1500]]</td>\n",
       "      <td>0.809935</td>\n",
       "      <td>0.883913</td>\n",
       "      <td>5.963500</td>\n",
       "      <td>412.343000</td>\n",
       "      <td>12.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.343600</td>\n",
       "      <td>7.346102</td>\n",
       "      <td>0.650802</td>\n",
       "      <td>0.787312</td>\n",
       "      <td>0.854033</td>\n",
       "      <td>0.717200</td>\n",
       "      <td>0.731136</td>\n",
       "      <td>0.212688</td>\n",
       "      <td>0.608240</td>\n",
       "      <td>[[ 406  356]\n",
       " [ 167 1530]]</td>\n",
       "      <td>0.811241</td>\n",
       "      <td>0.901591</td>\n",
       "      <td>6.019700</td>\n",
       "      <td>408.493000</td>\n",
       "      <td>12.791000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>7.374184</td>\n",
       "      <td>0.601199</td>\n",
       "      <td>0.786499</td>\n",
       "      <td>0.850979</td>\n",
       "      <td>0.727095</td>\n",
       "      <td>0.737318</td>\n",
       "      <td>0.213501</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>[[ 435  327]\n",
       " [ 198 1499]]</td>\n",
       "      <td>0.820920</td>\n",
       "      <td>0.883324</td>\n",
       "      <td>6.011800</td>\n",
       "      <td>409.032000</td>\n",
       "      <td>12.808000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2459\n",
      "  Batch size = 32\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2459\n",
      "  Batch size = 32\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2459\n",
      "  Batch size = 32\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2459\n",
      "  Batch size = 32\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18979\n",
      "  Batch size = 32\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='594' max='594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [594/594 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on test set: {'eval_loss': 0.3937152624130249, 'eval_accuracy': 0.8365035038727014, 'eval_f1': 0.8882164343095933, 'eval_balanced_accuracy': 0.8190740182675251, 'eval_f1_macro': 0.7919854600217444, 'eval_mean_squared_error': 0.16349649612729858, 'eval_f1_binary': 0.6957544857338954, 'eval_confusion_matrix': '[[ 3548   967]\\n [ 2136 12328]]', 'eval_precision': 0.9272658894321173, 'eval_recall': 0.8523230088495575, 'eval_log_loss': 5.647009661421722, 'eval_runtime': 41.1289, 'eval_samples_per_second': 461.451, 'eval_steps_per_second': 14.442, 'epoch': 3.0}\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "launch = {\n",
    "'distilbert': 'distilbert-base-uncased',\n",
    "'deberta': 'microsoft/deberta-base',\n",
    "'electra': 'google/electra-base-discriminator',\n",
    "}\n",
    " \n",
    "experiment = list(launch.keys())[2]+\"_clickbait\"\n",
    "model = launch['electra']\n",
    "\n",
    "train(model,root,nLabels,labels,experiment,trainDataset,validationDataset,testDataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ba5ff17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'accuracy': 0.7568117120780805, 'f1': 0.8270676691729323, 'balanced_accuracy': 0.7041401608829538, 'f1_macro': 0.708739314038521, 'mean_squared_error': 0.2431882879219195, 'f1_binary': 0.5904109589041096, 'confusion_matrix': '[[ 431  331]\\n [ 267 1430]]', 'precision': 0.8120386144236229, 'recall': 0.8426635238656452, 'log_loss': 8.399533530258635}, {'accuracy': 0.7616917446116307, 'f1': 0.8304398148148148, 'balanced_accuracy': 0.7102065247147583, 'f1_macro': 0.7148095106905811, 'mean_squared_error': 0.23830825538836925, 'f1_binary': 0.5991792065663474, 'confusion_matrix': '[[ 438  324]\\n [ 262 1435]]', 'precision': 0.8158044343376919, 'recall': 0.8456098998232174, 'log_loss': 8.230980901579928}, {'accuracy': 0.7881252541683611, 'f1': 0.8529494778436353, 'balanced_accuracy': 0.7253811342232781, 'f1_macro': 0.7370201934672722, 'mean_squared_error': 0.21187474583163887, 'f1_binary': 0.621090909090909, 'confusion_matrix': '[[ 427  335]\\n [ 186 1511]]', 'precision': 0.8185265438786565, 'recall': 0.8903948143783147, 'log_loss': 7.3180034025569665}, {'accuracy': 0.7812118747458316, 'f1': 0.8474191718661372, 'balanced_accuracy': 0.7203722950954055, 'f1_macro': 0.730462459496287, 'mean_squared_error': 0.21878812525416835, 'f1_binary': 0.6135057471264368, 'confusion_matrix': '[[ 427  335]\\n [ 203 1494]]', 'precision': 0.8168398031711318, 'recall': 0.8803771361225693, 'log_loss': 7.556783068564889}, {'accuracy': 0.8390326149955214, 'f1': 0.8899376733796881, 'balanced_accuracy': 0.8226376449813306, 'f1_macro': 0.7952286151429369, 'mean_squared_error': 0.16096738500447863, 'f1_binary': 0.7005195569061857, 'confusion_matrix': '[[ 3573   942]\\n [ 2113 12351]]', 'precision': 0.9291356352967728, 'recall': 0.8539131637168141, 'log_loss': 5.5596562046067435}, {'accuracy': 0.7612850752338349, 'f1': 0.8357122865938987, 'balanced_accuracy': 0.6885815945075221, 'f1_macro': 0.6996405299140498, 'mean_squared_error': 0.2387149247661651, 'f1_binary': 0.5635687732342007, 'confusion_matrix': '[[ 379  383]\\n [ 204 1493]]', 'precision': 0.7958422174840085, 'recall': 0.8797878609310548, 'log_loss': 8.24504594942185}, {'accuracy': 0.7767385115900772, 'f1': 0.8453085376162299, 'balanced_accuracy': 0.7109852650269041, 'f1_macro': 0.7221429466751712, 'mean_squared_error': 0.22326148840992274, 'f1_binary': 0.5989773557341125, 'confusion_matrix': '[[ 410  352]\\n [ 197 1500]]', 'precision': 0.8099352051835853, 'recall': 0.8839127872716559, 'log_loss': 7.711293086255142}, {'accuracy': 0.7873119154127695, 'f1': 0.8540329332961206, 'balanced_accuracy': 0.7171997209836101, 'f1_macro': 0.7311363168353262, 'mean_squared_error': 0.21268808458723057, 'f1_binary': 0.6082397003745318, 'confusion_matrix': '[[ 406  356]\\n [ 167 1530]]', 'precision': 0.8112407211028632, 'recall': 0.901591043017089, 'log_loss': 7.346101956577241}, {'accuracy': 0.7864985766571777, 'f1': 0.8509792790235595, 'balanced_accuracy': 0.7270948269062124, 'f1_macro': 0.737317596501027, 'mean_squared_error': 0.2135014233428223, 'f1_binary': 0.6236559139784946, 'confusion_matrix': '[[ 435  327]\\n [ 198 1499]]', 'precision': 0.8209200438116101, 'recall': 0.8833235120801414, 'log_loss': 7.374184252008045}, {'accuracy': 0.8365035038727014, 'f1': 0.8882164343095933, 'balanced_accuracy': 0.8190740182675251, 'f1_macro': 0.7919854600217444, 'mean_squared_error': 0.16349649612729858, 'f1_binary': 0.6957544857338954, 'confusion_matrix': '[[ 3548   967]\\n [ 2136 12328]]', 'precision': 0.9272658894321173, 'recall': 0.8523230088495575, 'log_loss': 5.647009661421722}]\n"
     ]
    }
   ],
   "source": [
    "print(all_computeMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8571d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'eval_loss': 0.38286539912223816, 'eval_accuracy': 0.8390326149955214, 'eval_f1': 0.8899376733796881, 'eval_balanced_accuracy': 0.8226376449813306, 'eval_f1_macro': 0.7952286151429369, 'eval_mean_squared_error': 0.16096738500447863, 'eval_f1_binary': 0.7005195569061857, 'eval_confusion_matrix': '[[ 3573   942]\\n [ 2113 12351]]', 'eval_precision': 0.9291356352967728, 'eval_recall': 0.8539131637168141, 'eval_log_loss': 5.5596562046067435, 'eval_runtime': 41.1735, 'eval_samples_per_second': 460.952, 'eval_steps_per_second': 14.427, 'epoch': 3.0}, {'eval_loss': 0.3937152624130249, 'eval_accuracy': 0.8365035038727014, 'eval_f1': 0.8882164343095933, 'eval_balanced_accuracy': 0.8190740182675251, 'eval_f1_macro': 0.7919854600217444, 'eval_mean_squared_error': 0.16349649612729858, 'eval_f1_binary': 0.6957544857338954, 'eval_confusion_matrix': '[[ 3548   967]\\n [ 2136 12328]]', 'eval_precision': 0.9272658894321173, 'eval_recall': 0.8523230088495575, 'eval_log_loss': 5.647009661421722, 'eval_runtime': 41.1289, 'eval_samples_per_second': 461.451, 'eval_steps_per_second': 14.442, 'epoch': 3.0}]\n"
     ]
    }
   ],
   "source": [
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a08d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
