{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d1e416",
   "metadata": {},
   "source": [
    "#### Clickbait Detection by usiing Headline's text using Deberta and Electra models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991c9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, balanced_accuracy_score, mean_squared_error, confusion_matrix, recall_score, precision_score, accuracy_score, log_loss\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig,Trainer, TrainingArguments, BertweetTokenizer, BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c62dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Webis17HeadlineClsData/'  # Webis17HeadlineClsData or Webis17HeadlineArticleClsData\n",
    "root = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e66f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path+'train.csv')\n",
    "df_validation = pd.read_csv(path+'validation.csv')\n",
    "df_test = pd.read_csv(path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd7717a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postText</th>\n",
       "      <th>truthClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK’s response to modern slavery leaving victim...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is good</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The \"forgotten\" Trump roast: Relive his brutal...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meet the happiest #dog in the world!</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tokyo's subway is shut down amid fears over an...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            postText    truthClass\n",
       "0  UK’s response to modern slavery leaving victim...  no-clickbait\n",
       "1                                       this is good     clickbait\n",
       "2  The \"forgotten\" Trump roast: Relive his brutal...  no-clickbait\n",
       "3               Meet the happiest #dog in the world!     clickbait\n",
       "4  Tokyo's subway is shut down amid fears over an...  no-clickbait"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb618e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19538, 2459, 18979)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_validation), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0831db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(sentence, replace_url=False, replace_usr=False):\n",
    "    if not replace_url and not replace_usr:\n",
    "        #print(replace_url,replace_usr)\n",
    "        return sentence\n",
    "    new_sentence = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        # print(word)\n",
    "        # @mentions, @users\n",
    "        if (word.startswith(\"@\") or word.startswith('\"@')) and replace_usr:\n",
    "            new_sentence.append('@user')\n",
    "        # URL: https, http\n",
    "        elif (word.startswith(\"http:\") or word.startswith(\"https:\")) and replace_url:\n",
    "            new_sentence.append('internet_site')\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    # remove extra \" \"\n",
    "    new_sentence = re.sub(' +', ' ', \" \".join(new_sentence))\n",
    "    return new_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565ae6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of train 19538\n",
      "Total amount of validation 2459\n",
      "Total amount of test 18979\n",
      "Labels: ['clickbait', 'no-clickbait']\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(df_train,df_dev,df_test,root,replace_url=False,replace_usr=False):\n",
    "    trainDataset = df_train[['postText', 'truthClass']].copy()\n",
    "    trainDataset['x'] = trainDataset.postText.astype(str).apply(lambda a: normalize_sentence(a,replace_url,replace_usr))\n",
    "    trainDataset['y'] = trainDataset.truthClass.str.strip()\n",
    "    print('Total amount of train',len(trainDataset.index))\n",
    "\n",
    "    validationDataset = df_dev[['postText', 'truthClass']].copy()\n",
    "    validationDataset['x'] = validationDataset.postText.astype(str).apply(lambda a: normalize_sentence(a,replace_url,replace_usr))\n",
    "    validationDataset['y'] = validationDataset.truthClass.str.strip()\n",
    "    print('Total amount of validation',len(validationDataset.index))\n",
    "\n",
    "    testDataset = df_test[['postText', 'truthClass']].copy()\n",
    "    testDataset['x'] = testDataset.postText.astype(str).apply(lambda a: normalize_sentence(a,replace_url,replace_usr))\n",
    "    testDataset['y'] = testDataset.truthClass.str.strip()\n",
    "    print('Total amount of test',len(testDataset.index))\n",
    "\n",
    "    return trainDataset, validationDataset, testDataset\n",
    "\n",
    "trainDataset, validationDataset, testDataset = get_dataset(df_train, df_validation, df_test, \".\", True, True)\n",
    "\n",
    "labels = list(set(trainDataset.truthClass.tolist()))\n",
    "labels.sort(key=lambda item: (-len(item), item), reverse=True)\n",
    "nLabels = len(labels)\n",
    "print('Labels:', labels)\n",
    "\n",
    "trainDataset = trainDataset.drop(['postText', 'truthClass'], axis=1)\n",
    "validationDataset = validationDataset.drop(['postText', 'truthClass'], axis=1)\n",
    "testDataset = testDataset.drop(['postText', 'truthClass'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1886c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   x             y\n",
      "0  Johnny Manziel on Browns' No. 1 pick Myles Gar...  no-clickbait\n",
      "1  Fabio: California Is a 'Mess' Because of Liber...  no-clickbait\n",
      "2            \"He's been huge for us this year, man.\"     clickbait\n",
      "3  New Bears quarterback Mitchell Trubisky was gr...  no-clickbait\n",
      "4  It's not enough to let employees work flexible...  no-clickbait\n"
     ]
    }
   ],
   "source": [
    "print(testDataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c845012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(Trainer):\n",
    "    def evaluate(self, eval_dataset= None, ignore_keys=None):\n",
    "        outputMetrics = super().evaluate(eval_dataset, ignore_keys)\n",
    "#         print('outputMetrics: ', outputMetrics)\n",
    "        return outputMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ec7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gn_dataset(Dataset):\n",
    "    def __init__(self,data,labels,tokenizer):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def processText(self, text):\n",
    "        tokenized = self.tokenizer(text, truncation=True)\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.index)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        row = self.data.iloc[i]\n",
    "        x = self.processText(row['x']).data\n",
    "\n",
    "        try:\n",
    "            y = self.labels.index(row['y'])\n",
    "        except:\n",
    "            y = len(self.labels) - 1 \n",
    "\n",
    "        x['labels'] = y\n",
    "        return x\n",
    "\n",
    "    def randomItem(self):\n",
    "        return self.__getitem__(random.randint(0,self.__len__()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beebf692",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_computeMetrics = []\n",
    "test_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36690d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,modelPath = '',nLabels = 3, labels=None):\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # put cpu for more extensive error desc...\n",
    "        print('device: ', self.device)\n",
    "        self.maxLength = 1024  # max len we have is 124 for postext\n",
    "\n",
    "        self.nLabels = nLabels\n",
    "\n",
    "        self.loadModel(modelPath) \n",
    "        print('** Model: ', self.loadModel)\n",
    "\n",
    "        self.labels = labels\n",
    "\n",
    "    def model_init(self, dropout = 0.1):\n",
    "        config = AutoConfig.from_pretrained(\n",
    "                    self.MODEL_PATH,\n",
    "                    num_labels=self.nLabels,\n",
    "                    return_dict = True,\n",
    "                    hidden_dropout_prob = dropout\n",
    "                )\n",
    "\n",
    "        print('configconfig', config)\n",
    "\n",
    "        #return AutoModel.from_pretrained(self.MODEL_PATH,config=config).to(self.device)\n",
    "        return AutoModelForSequenceClassification.from_pretrained(self.MODEL_PATH,config=config).to(self.device)\n",
    "\n",
    "    def computeMetrics(self,evalPrediction):\n",
    "        yPred = evalPrediction.predictions.argmax(1)\n",
    "        yTrue = evalPrediction.label_ids\n",
    "\n",
    "        metrics = {}\n",
    "        \n",
    "        metrics['accuracy'] = accuracy_score(yTrue, yPred)\n",
    "        metrics['f1'] = f1_score(yTrue, yPred)\n",
    "\n",
    "        # original paper: https://link.springer.com/chapter/10.1007/978-3-319-30671-1_72/tables/2 / ROC-AUC, Precision, Recall\n",
    "        metrics['balanced_accuracy'] = balanced_accuracy_score(yTrue, yPred) # deal with imbalanced datasets\n",
    "        metrics['f1_macro'] = f1_score(yTrue, yPred, average='macro') # Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "        metrics['mean_squared_error'] = mean_squared_error(yTrue, yPred)\n",
    "        if 'click' in self.labels[0]:\n",
    "            indexes = [i for i,x in enumerate(self.labels) if x == 'clickbait']\n",
    "            metrics['f1_binary'] = f1_score(yTrue, yPred, average='binary',pos_label=indexes[0])\n",
    "        metrics['confusion_matrix'] = str(confusion_matrix(yTrue, yPred))\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(yTrue, yPred).ravel()\n",
    "        recall = recall_score(yTrue, yPred) \n",
    "        precision = precision_score(yTrue, yPred)\n",
    "        metrics['precision'] = precision\n",
    "        metrics['recall'] = recall\n",
    "        metrics['log_loss'] = log_loss(yTrue, yPred)\n",
    "        \n",
    "        all_computeMetrics.append(metrics)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def loadModel(self,modelPath):\n",
    "        self.MODEL_PATH = modelPath\n",
    "        self.MODEL = self.model_init()\n",
    "        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_PATH,model_max_length = self.maxLength, use_fast=True)\n",
    "        \n",
    "    def saveModel(self,saveDir):\n",
    "        print('saving model at: ', saveDir)\n",
    "        self.MODEL.save_pretrained(saveDir)\n",
    "        self.TOKENIZER.save_pretrained(saveDir)\n",
    "\n",
    "    def train_loop(self,saveDir,checkpointDir,trainingData,validationData,testData,labels):\n",
    "        trainDataset = gn_dataset(trainingData,labels,self.TOKENIZER)\n",
    "        validationDataset = gn_dataset(validationData,labels,self.TOKENIZER)\n",
    "        testDataset = gn_dataset(testData,labels,self.TOKENIZER)\n",
    "        checkpoints = {}\n",
    "\n",
    "        # TRAIN FUNCTION\n",
    "        runName = experiment + str(len(checkpoints))\n",
    "        print('checkpointDir: ', checkpointDir)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "                                output_dir=checkpointDir,\n",
    "                                save_strategy='no',\n",
    "                                do_train=True, # DEFAULT: False\n",
    "                                do_eval=True, # DEFAULT: False\n",
    "                                do_predict=True, # DEFAULT: False\n",
    "#                                 save_steps = 1200, # DEFAULT: 500\n",
    "                                eval_steps = 400, #200, # DEFAULT: 500\n",
    "                                evaluation_strategy = 'steps', # DEFAULT: \"no\"\n",
    "                                logging_first_step = True, # DEFAULT: False\n",
    "                                dataloader_num_workers = 12, # 6, DEFAULT: 0\n",
    "                                learning_rate = 2e-5,\n",
    "                                num_train_epochs = 3,\n",
    "                                per_device_train_batch_size = 32,\n",
    "                                per_device_eval_batch_size = 32, #16,\n",
    "                                weight_decay = 0.05 ,\n",
    "                                warmup_steps = 0, # DEFAULT: 0\n",
    "                                logging_dir=checkpointDir+\"/logs\",\n",
    "                                metric_for_best_model = 'f1_binary', # DEFAULT: None\n",
    "                                greater_is_better = True, # DEFAULT: None\n",
    "                                \n",
    "        )\n",
    "        model = self.model_init(dropout=0.3)\n",
    "\n",
    "        trainer = Trainer(\n",
    "                    model,\n",
    "                    args = args,\n",
    "                    train_dataset = trainDataset,#train\n",
    "                    tokenizer = self.TOKENIZER,\n",
    "                    eval_dataset = validationDataset,#dev\n",
    "                    compute_metrics = self.computeMetrics,\n",
    "                )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        cp = checkpointDir + '/CheckPointModel_'+ runName\n",
    "\n",
    "        checkpoints[cp] = trainer.evaluate(testDataset) # on test set\n",
    "        print('on test set:', checkpoints[cp]) \n",
    "        test_metrics.append(checkpoints[cp])\n",
    "#         trainer.predict(testDataset)\n",
    "\n",
    "#         trainer.save_model(cp)\n",
    "        \n",
    "    def predict(self,text):\n",
    "        dataset = gn_dataset(None,None,self.TOKENIZER)\n",
    "        batchEncoding = dataset.processText(text).to(self.device)\n",
    "\n",
    "        self.MODEL.eval()\n",
    "        out = self.MODEL(batchEncoding.input_ids,attention_mask = batchEncoding.attention_mask,token_type_ids = batchEncoding.token_type_ids,return_dict = True)\n",
    "\n",
    "        return out.logits.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d586eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(name,root,nLabels,labels,experiment,trainDataset,validationDataset,testDataset): \n",
    "    print('name experiment: ', name, experiment)\n",
    "    model = Model(name,nLabels,labels)\n",
    "    print('model model: ', model)\n",
    "    analysis = model.train_loop(root+\"SavedModel/\"+experiment,root+name.split('/')[-1]+\"TrainingCheckpoints\",trainDataset,validationDataset,testDataset,labels)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "735de030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name experiment:  google/electra-base-discriminator electra_clickbait\n",
      "device:  cuda\n",
      "configconfig ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Model:  <bound method Model.loadModel of <__main__.Model object at 0x7f533f0804a8>>\n",
      "model model:  <__main__.Model object at 0x7f533f0804a8>\n",
      "checkpointDir:  ./electra-base-discriminatorTrainingCheckpoints\n",
      "configconfig ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/charmichokshi4444/venv/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 19538\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1833\n",
      "  Number of trainable parameters = 109483778\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1833' max='1833' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1833/1833 02:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "      <th>F1 Binary</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.686100</td>\n",
       "      <td>7.514677</td>\n",
       "      <td>0.579633</td>\n",
       "      <td>0.782432</td>\n",
       "      <td>0.856299</td>\n",
       "      <td>0.686188</td>\n",
       "      <td>0.704300</td>\n",
       "      <td>0.217568</td>\n",
       "      <td>0.552301</td>\n",
       "      <td>[[ 330  432]\n",
       " [ 103 1594]]</td>\n",
       "      <td>0.786772</td>\n",
       "      <td>0.939305</td>\n",
       "      <td>2.010300</td>\n",
       "      <td>1223.227000</td>\n",
       "      <td>38.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>7.570859</td>\n",
       "      <td>0.610022</td>\n",
       "      <td>0.780805</td>\n",
       "      <td>0.854599</td>\n",
       "      <td>0.687178</td>\n",
       "      <td>0.704756</td>\n",
       "      <td>0.219195</td>\n",
       "      <td>0.554913</td>\n",
       "      <td>[[ 336  426]\n",
       " [ 113 1584]]</td>\n",
       "      <td>0.788060</td>\n",
       "      <td>0.933412</td>\n",
       "      <td>2.034800</td>\n",
       "      <td>1208.497000</td>\n",
       "      <td>37.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>7.444445</td>\n",
       "      <td>0.616129</td>\n",
       "      <td>0.784465</td>\n",
       "      <td>0.857220</td>\n",
       "      <td>0.690553</td>\n",
       "      <td>0.708875</td>\n",
       "      <td>0.215535</td>\n",
       "      <td>0.560531</td>\n",
       "      <td>[[ 338  424]\n",
       " [ 106 1591]]</td>\n",
       "      <td>0.789578</td>\n",
       "      <td>0.937537</td>\n",
       "      <td>2.011100</td>\n",
       "      <td>1222.731000</td>\n",
       "      <td>38.288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.326800</td>\n",
       "      <td>7.289932</td>\n",
       "      <td>0.571203</td>\n",
       "      <td>0.788939</td>\n",
       "      <td>0.858622</td>\n",
       "      <td>0.703194</td>\n",
       "      <td>0.721211</td>\n",
       "      <td>0.211061</td>\n",
       "      <td>0.583801</td>\n",
       "      <td>[[ 364  398]\n",
       " [ 121 1576]]</td>\n",
       "      <td>0.798379</td>\n",
       "      <td>0.928698</td>\n",
       "      <td>1.996600</td>\n",
       "      <td>1231.594000</td>\n",
       "      <td>38.566000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2459\n",
      "  Batch size = 32\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2459\n",
      "  Batch size = 32\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2459\n",
      "  Batch size = 32\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2459\n",
      "  Batch size = 32\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18979\n",
      "  Batch size = 32\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='594' max='594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [594/594 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on test set: {'eval_loss': 0.3154461979866028, 'eval_accuracy': 0.8665893882712472, 'eval_f1': 0.9128099173553719, 'eval_balanced_accuracy': 0.8117711273385666, 'eval_f1_macro': 0.814444877942095, 'eval_mean_squared_error': 0.13341061172875282, 'eval_f1_binary': 0.7160798385288181, 'eval_confusion_matrix': '[[ 3193  1322]\\n [ 1210 13254]]', 'eval_precision': 0.9093029637760702, 'eval_recall': 0.9163440265486725, 'eval_log_loss': 4.607894983914673, 'eval_runtime': 11.4341, 'eval_samples_per_second': 1659.856, 'eval_steps_per_second': 51.95, 'epoch': 3.0}\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "launch = {\n",
    "'distilbert': 'distilbert-base-uncased',\n",
    "'deberta': 'microsoft/deberta-base',\n",
    "'electra': 'google/electra-base-discriminator',\n",
    "}\n",
    " \n",
    "experiment = list(launch.keys())[2]+\"_clickbait\"\n",
    "model = launch['electra']\n",
    "\n",
    "train(model,root,nLabels,labels,experiment,trainDataset,validationDataset,testDataset) \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba5ff17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'accuracy': 0.7824318828792192, 'f1': 0.8562986838571045, 'balanced_accuracy': 0.6861877607078726, 'f1_macro': 0.704299969543615, 'mean_squared_error': 0.2175681171207808, 'f1_binary': 0.5523012552301255, 'confusion_matrix': '[[ 330  432]\\n [ 103 1594]]', 'precision': 0.786771964461994, 'recall': 0.939304655274013, 'log_loss': 7.514677022109419}, {'accuracy': 0.7808052053680358, 'f1': 0.8545994065281899, 'balanced_accuracy': 0.6871783926243162, 'f1_macro': 0.7047563506629388, 'mean_squared_error': 0.2191947946319642, 'f1_binary': 0.5549132947976878, 'confusion_matrix': '[[ 336  426]\\n [ 113 1584]]', 'precision': 0.7880597014925373, 'recall': 0.9334119033588686, 'log_loss': 7.570858521904077}, {'accuracy': 0.7844652297681984, 'f1': 0.8572198275862069, 'balanced_accuracy': 0.690553191752622, 'f1_macro': 0.708875253759936, 'mean_squared_error': 0.21553477023180154, 'f1_binary': 0.5605306799336651, 'confusion_matrix': '[[ 338  424]\\n [ 106 1591]]', 'precision': 0.7895781637717122, 'recall': 0.9375368296994696, 'log_loss': 7.444445107203361}, {'accuracy': 0.7889385929239529, 'f1': 0.858621628983928, 'balanced_accuracy': 0.7031939952703319, 'f1_macro': 0.7212113758391974, 'mean_squared_error': 0.21106140707604717, 'f1_binary': 0.5838011226944667, 'confusion_matrix': '[[ 364  398]\\n [ 121 1576]]', 'precision': 0.7983789260385005, 'recall': 0.928697701826753, 'log_loss': 7.289932162967004}, {'accuracy': 0.8665893882712472, 'f1': 0.9128099173553719, 'balanced_accuracy': 0.8117711273385666, 'f1_macro': 0.814444877942095, 'mean_squared_error': 0.13341061172875282, 'f1_binary': 0.7160798385288181, 'confusion_matrix': '[[ 3193  1322]\\n [ 1210 13254]]', 'precision': 0.9093029637760702, 'recall': 0.9163440265486725, 'log_loss': 4.607894983914673}]\n"
     ]
    }
   ],
   "source": [
    "print(all_computeMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8571d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'eval_loss': 0.3154461979866028, 'eval_accuracy': 0.8665893882712472, 'eval_f1': 0.9128099173553719, 'eval_balanced_accuracy': 0.8117711273385666, 'eval_f1_macro': 0.814444877942095, 'eval_mean_squared_error': 0.13341061172875282, 'eval_f1_binary': 0.7160798385288181, 'eval_confusion_matrix': '[[ 3193  1322]\\n [ 1210 13254]]', 'eval_precision': 0.9093029637760702, 'eval_recall': 0.9163440265486725, 'eval_log_loss': 4.607894983914673, 'eval_runtime': 11.4341, 'eval_samples_per_second': 1659.856, 'eval_steps_per_second': 51.95, 'epoch': 3.0}]\n"
     ]
    }
   ],
   "source": [
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a08d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
