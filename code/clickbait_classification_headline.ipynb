{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d1e416",
   "metadata": {},
   "source": [
    "#### Clickbait Detection on Headline's text using Deberta and Electra pre-trained Models and their Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991c9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, balanced_accuracy_score, mean_squared_error, confusion_matrix, recall_score, precision_score, accuracy_score, log_loss\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig,Trainer, TrainingArguments, BertweetTokenizer, BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c62dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Webis17HeadlineClsData/'  # Webis17HeadlineClsData or Webis17HeadlineArticleClsData\n",
    "root = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e66f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path+'train.csv')\n",
    "df_validation = pd.read_csv(path+'validation.csv')\n",
    "df_test = pd.read_csv(path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd7717a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postText</th>\n",
       "      <th>truthClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK’s response to modern slavery leaving victim...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is good</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The \"forgotten\" Trump roast: Relive his brutal...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meet the happiest #dog in the world!</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tokyo's subway is shut down amid fears over an...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            postText    truthClass\n",
       "0  UK’s response to modern slavery leaving victim...  no-clickbait\n",
       "1                                       this is good     clickbait\n",
       "2  The \"forgotten\" Trump roast: Relive his brutal...  no-clickbait\n",
       "3               Meet the happiest #dog in the world!     clickbait\n",
       "4  Tokyo's subway is shut down amid fears over an...  no-clickbait"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb618e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19538, 2459, 18979)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_validation), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0831db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(sentence, replace_url=False, replace_usr=False):\n",
    "    if not replace_url and not replace_usr:\n",
    "        #print(replace_url,replace_usr)\n",
    "        return sentence\n",
    "    new_sentence = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        # print(word)\n",
    "        # @mentions, @users\n",
    "        if (word.startswith(\"@\") or word.startswith('\"@')) and replace_usr:\n",
    "            new_sentence.append('@user')\n",
    "        # URL: https, http\n",
    "        elif (word.startswith(\"http:\") or word.startswith(\"https:\")) and replace_url:\n",
    "            new_sentence.append('internet_site')\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "    # remove extra \" \"\n",
    "    new_sentence = re.sub(' +', ' ', \" \".join(new_sentence))\n",
    "    return new_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565ae6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of train 19538\n",
      "Total amount of validation 2459\n",
      "Total amount of test 18979\n",
      "Labels: ['clickbait', 'no-clickbait']\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(df_train,df_dev,df_test,root,replace_url=False,replace_usr=False):\n",
    "    trainDataset = df_train[['postText', 'truthClass']].copy()\n",
    "    trainDataset['x'] = trainDataset.postText.astype(str).apply(lambda a: normalize_sentence(a,replace_url,replace_usr))\n",
    "    trainDataset['y'] = trainDataset.truthClass.str.strip()\n",
    "    print('Total amount of train',len(trainDataset.index))\n",
    "\n",
    "    validationDataset = df_dev[['postText', 'truthClass']].copy()\n",
    "    validationDataset['x'] = validationDataset.postText.astype(str).apply(lambda a: normalize_sentence(a,replace_url,replace_usr))\n",
    "    validationDataset['y'] = validationDataset.truthClass.str.strip()\n",
    "    print('Total amount of validation',len(validationDataset.index))\n",
    "\n",
    "    testDataset = df_test[['postText', 'truthClass']].copy()\n",
    "    testDataset['x'] = testDataset.postText.astype(str).apply(lambda a: normalize_sentence(a,replace_url,replace_usr))\n",
    "    testDataset['y'] = testDataset.truthClass.str.strip()\n",
    "    print('Total amount of test',len(testDataset.index))\n",
    "\n",
    "    return trainDataset, validationDataset, testDataset\n",
    "\n",
    "trainDataset, validationDataset, testDataset = get_dataset(df_train, df_validation, df_test, \".\", True, True)\n",
    "\n",
    "labels = list(set(trainDataset.truthClass.tolist()))\n",
    "labels.sort(key=lambda item: (-len(item), item), reverse=True)\n",
    "nLabels = len(labels)\n",
    "print('Labels:', labels)\n",
    "\n",
    "trainDataset = trainDataset.drop(['postText', 'truthClass'], axis=1)\n",
    "validationDataset = validationDataset.drop(['postText', 'truthClass'], axis=1)\n",
    "testDataset = testDataset.drop(['postText', 'truthClass'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1886c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   x             y\n",
      "0  Johnny Manziel on Browns' No. 1 pick Myles Gar...  no-clickbait\n",
      "1  Fabio: California Is a 'Mess' Because of Liber...  no-clickbait\n",
      "2            \"He's been huge for us this year, man.\"     clickbait\n",
      "3  New Bears quarterback Mitchell Trubisky was gr...  no-clickbait\n",
      "4  It's not enough to let employees work flexible...  no-clickbait\n"
     ]
    }
   ],
   "source": [
    "print(testDataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c845012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(Trainer):\n",
    "    def evaluate(self, eval_dataset= None, ignore_keys=None):\n",
    "        outputMetrics = super().evaluate(eval_dataset, ignore_keys)\n",
    "#         print('outputMetrics: ', outputMetrics)\n",
    "        return outputMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ec7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gn_dataset(Dataset):\n",
    "    def __init__(self,data,labels,tokenizer):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def processText(self, text):\n",
    "        tokenized = self.tokenizer(text, truncation=True)\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.index)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        row = self.data.iloc[i]\n",
    "        x = self.processText(row['x']).data\n",
    "\n",
    "        try:\n",
    "            y = self.labels.index(row['y'])\n",
    "        except:\n",
    "            y = len(self.labels) - 1 \n",
    "\n",
    "        x['labels'] = y\n",
    "        return x\n",
    "\n",
    "    def randomItem(self):\n",
    "        return self.__getitem__(random.randint(0,self.__len__()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beebf692",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_computeMetrics = []\n",
    "test_metrics = []\n",
    "\n",
    "# y_pred = []\n",
    "# y_true = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36690d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,modelPath = '',nLabels = 3, labels=None):\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # put cpu for more extensive error desc...\n",
    "        print('device: ', self.device)\n",
    "        self.maxLength = 128  # max len we have is 124 for postext\n",
    "        self.nLabels = nLabels\n",
    "        self.loadModel(modelPath) \n",
    "        print('** Model: ', self.loadModel)\n",
    "\n",
    "        self.labels = labels\n",
    "\n",
    "    def model_init(self, dropout = 0.1):\n",
    "        config = AutoConfig.from_pretrained(\n",
    "                    self.MODEL_PATH,\n",
    "                    num_labels=self.nLabels,\n",
    "                    return_dict = True,\n",
    "                    hidden_dropout_prob = dropout\n",
    "                )\n",
    "        print('configconfig', config)\n",
    "        return AutoModelForSequenceClassification.from_pretrained(self.MODEL_PATH,config=config).to(self.device)\n",
    "\n",
    "    def computeMetrics(self,evalPrediction):\n",
    "        yPred = evalPrediction.predictions.argmax(1)\n",
    "        yTrue = evalPrediction.label_ids\n",
    "        \n",
    "#         y_pred.append(yPred)\n",
    "#         y_true.append(yTrue)\n",
    "\n",
    "        metrics = {}\n",
    "        \n",
    "        metrics['val accuracy'] = accuracy_score(yTrue, yPred)\n",
    "        metrics['f1'] = f1_score(yTrue, yPred)\n",
    "\n",
    "        # original paper: https://link.springer.com/chapter/10.1007/978-3-319-30671-1_72/tables/2 / ROC-AUC, Precision, Recall\n",
    "#         metrics['balanced_accuracy'] = balanced_accuracy_score(yTrue, yPred) # deal with imbalanced datasets\n",
    "#         metrics['f1_macro'] = f1_score(yTrue, yPred, average='macro') # Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "#         metrics['mean_squared_error'] = mean_squared_error(yTrue, yPred)\n",
    "        \n",
    "#         if 'click' in self.labels[0]:\n",
    "#             indexes = [i for i,x in enumerate(self.labels) if x == 'clickbait']\n",
    "#             metrics['f1_binary'] = f1_score(yTrue, yPred, average='binary',pos_label=0)\n",
    "#         metrics['confusion_matrix'] = str(confusion_matrix(yTrue, yPred))\n",
    "        \n",
    "#         tn, fp, fn, tp = confusion_matrix(yTrue, yPred).ravel()\n",
    "#         recall = recall_score(yTrue, yPred) \n",
    "#         precision = precision_score(yTrue, yPred)\n",
    "#         metrics['precision'] = precision\n",
    "#         metrics['recall'] = recall\n",
    "#         metrics['log_loss'] = log_loss(yTrue, yPred)\n",
    "        \n",
    "        all_computeMetrics.append(metrics)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def loadModel(self,modelPath):\n",
    "        self.MODEL_PATH = modelPath\n",
    "        self.MODEL = self.model_init()\n",
    "        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_PATH,model_max_length = self.maxLength, use_fast=True)\n",
    "        \n",
    "    def saveModel(self,saveDir):\n",
    "        print('saving model at: ', saveDir)\n",
    "        self.MODEL.save_pretrained(saveDir)\n",
    "        self.TOKENIZER.save_pretrained(saveDir)\n",
    "\n",
    "    def train_loop(self,saveDir,checkpointDir,trainingData,validationData,testData,labels):\n",
    "        trainDataset = gn_dataset(trainingData,labels,self.TOKENIZER)\n",
    "        validationDataset = gn_dataset(validationData,labels,self.TOKENIZER)\n",
    "        testDataset = gn_dataset(testData,labels,self.TOKENIZER)\n",
    "        checkpoints = {}\n",
    "\n",
    "        # TRAIN FUNCTION\n",
    "        runName = experiment + str(len(checkpoints))\n",
    "        print('checkpointDir: ', checkpointDir)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "                                output_dir=checkpointDir,\n",
    "                                save_strategy='no',\n",
    "                                do_train=True, # DEFAULT: False\n",
    "                                do_eval=True, # DEFAULT: False\n",
    "                                do_predict=True, # DEFAULT: False\n",
    "#                                 save_steps = 1200, # DEFAULT: 500\n",
    "#                                 eval_steps = 306, #200, # DEFAULT: 500\n",
    "                                evaluation_strategy = 'epoch', # DEFAULT: \"no\"\n",
    "                                logging_first_step = True, # DEFAULT: False\n",
    "                                dataloader_num_workers = 12, # 6, DEFAULT: 0\n",
    "                                learning_rate = 2e-5,\n",
    "                                num_train_epochs = 3,\n",
    "                                per_device_train_batch_size = 32,\n",
    "                                per_device_eval_batch_size = 32, #16,\n",
    "                                weight_decay = 0.05 ,\n",
    "                                warmup_steps = 0, # DEFAULT: 0\n",
    "                                logging_dir=checkpointDir+\"/logs\",\n",
    "                                metric_for_best_model = 'f1_binary', # DEFAULT: None\n",
    "                                greater_is_better = True, # DEFAULT: None\n",
    "                                \n",
    "        )\n",
    "        model = self.model_init(dropout=0.3)\n",
    "\n",
    "        trainer = Trainer(\n",
    "                    model,\n",
    "                    args = args,\n",
    "                    train_dataset = trainDataset,#train\n",
    "                    tokenizer = self.TOKENIZER,\n",
    "                    eval_dataset = validationDataset,#dev\n",
    "                    compute_metrics = self.computeMetrics,\n",
    "                )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        cp = checkpointDir + '/CheckPointModel_'+ runName\n",
    "\n",
    "        checkpoints[cp] = trainer.evaluate(testDataset) # on test set\n",
    "        print('on test set:', checkpoints[cp]) \n",
    "        test_metrics.append(checkpoints[cp])\n",
    "#         trainer.predict(testDataset)\n",
    "#         trainer.save_model(cp)\n",
    "        \n",
    "    def predict(self,text):\n",
    "        dataset = gn_dataset(None,None,self.TOKENIZER)\n",
    "        batchEncoding = dataset.processText(text).to(self.device)\n",
    "\n",
    "        self.MODEL.eval()\n",
    "        out = self.MODEL(batchEncoding.input_ids,attention_mask = batchEncoding.attention_mask,token_type_ids = batchEncoding.token_type_ids,return_dict = True)\n",
    "\n",
    "        return out.logits.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d586eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(name,root,nLabels,labels,experiment,trainDataset,validationDataset,testDataset): \n",
    "    print('name experiment: ', name, experiment)\n",
    "    model = Model(name,nLabels,labels)\n",
    "    print('model model: ', model)\n",
    "    analysis = model.train_loop(root+\"SavedModel/\"+experiment,root+name.split('/')[-1]+\"TrainingCheckpoints\",trainDataset,validationDataset,testDataset,labels)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "735de030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name experiment:  microsoft/deberta-base deberta_clickbait\n",
      "device:  cuda\n",
      "configconfig DebertaConfig {\n",
      "  \"_name_or_path\": \"microsoft/deberta-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Model:  <bound method Model.loadModel of <__main__.Model object at 0x7f57740bfac8>>\n",
      "model model:  <__main__.Model object at 0x7f57740bfac8>\n",
      "checkpointDir:  ./deberta-baseTrainingCheckpoints\n",
      "configconfig DebertaConfig {\n",
      "  \"_name_or_path\": \"microsoft/deberta-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/charmichokshi4444/venv/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 19538\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1833\n",
      "  Number of trainable parameters = 139193858\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/charmichokshi4444/venv/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py:1256: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  label_index = (labels >= 0).nonzero()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1833' max='1833' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1833/1833 04:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Val accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>0.471427</td>\n",
       "      <td>0.793412</td>\n",
       "      <td>0.859202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.326500</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>0.793412</td>\n",
       "      <td>0.864316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.634245</td>\n",
       "      <td>0.782432</td>\n",
       "      <td>0.859617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2459\n",
      "  Batch size = 32\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2459\n",
      "  Batch size = 32\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2459\n",
      "  Batch size = 32\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18979\n",
      "  Batch size = 32\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='594' max='594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [594/594 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on test set: {'eval_loss': 0.2994671165943146, 'eval_val accuracy': 0.8751251383107645, 'eval_f1': 0.9203789558556742, 'eval_runtime': 23.0572, 'eval_samples_per_second': 823.128, 'eval_steps_per_second': 25.762, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "launch = {\n",
    "'distilbert': 'distilbert-base-uncased',\n",
    "'deberta': 'microsoft/deberta-base',\n",
    "'electra': 'google/electra-base-discriminator',\n",
    "}\n",
    " \n",
    "experiment = list(launch.keys())[1]+\"_clickbait\"\n",
    "model = launch['deberta']\n",
    "\n",
    "train(model,root,nLabels,labels,experiment,trainDataset,validationDataset,testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba5ff17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'val accuracy': 0.7934119560797072, 'f1': 0.8592017738359202}, {'val accuracy': 0.7934119560797072, 'f1': 0.8643162393162394}, {'val accuracy': 0.7824318828792192, 'f1': 0.85961689845185}, {'val accuracy': 0.8751251383107645, 'f1': 0.9203789558556742}]\n"
     ]
    }
   ],
   "source": [
    "print(all_computeMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8571d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8751251383107645 \n",
      " 0.9203789558556742\n"
     ]
    }
   ],
   "source": [
    "print(test_metrics[0]['eval_val accuracy'], '\\n', test_metrics[0]['eval_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d8a08d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'eval_loss': 0.2994671165943146,\n",
       "  'eval_val accuracy': 0.8751251383107645,\n",
       "  'eval_f1': 0.9203789558556742,\n",
       "  'eval_runtime': 23.0572,\n",
       "  'eval_samples_per_second': 823.128,\n",
       "  'eval_steps_per_second': 25.762,\n",
       "  'epoch': 3.0}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fba55c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988dfe23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738727e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
